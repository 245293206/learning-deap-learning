---
  title: 2.二分类模型
  tags: deep learning，吴恩达，Binary Classification
  notebook: Deep Learning-eod
---

### 如何处理训练集

在神经网络的计算中，通常``先``有一个``前向传播``(前向暂停 forward pause)，``接着``有一个反向传播（反向暂停 backward pause）的步骤。

> 逻辑回归：一个用于二分类的算法

---
### 以猫的识别为例

> 图片（64X64像素）在计算机中的保存：三个64X64的``矩阵``,对应``R,G,B``三种像素的``强度值``。定义一个``特征向量``，线性存储所有像素值，总维度是``64X64X3``。

![RGB矩阵](https://img-blog.csdnimg.cn/20190508202555123.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2EyNDUyOTMyMDY=,size_16,color_FFFFFF,t_70)

- ``输入``是X（mXn<sub>x</sub>），``输出``Y（取值为0或1）

![X](https://img-blog.csdnimg.cn/2019050820401699.png)

![y](https://img-blog.csdnimg.cn/20190508204221237.png)

> ``列堆叠``要比行堆叠在进行神经网络训练时容易得多

- 假设函数（Hypothesis Function）
  - w：逻辑回归的参数（也是一个n<sub>x</sub>维的向量），``特征权重``
  - b：``偏差``（实数）

> 这在``线性回归``时可用到，``但``在进行``二分类``时，``无法``保证取值``范围``在``输出``值的``值域``中

> **采用sigmoid**函数

以逻辑回归的假设函数为``自变量z``
![sigmoid](https://img-blog.csdnimg.cn/20190508205102779.png)

![sigmoid图像](https://img-blog.csdnimg.cn/20190508205129609.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2EyNDUyOTMyMDY=,size_16,color_FFFFFF,t_70)

如果``z``趋于``正无穷``，sigmoid函数结果近似等于`1`，如果``z``趋于`负无穷`，sigmoid函数结果近似于`0`，保证了输出y在`（0,1）`之间

![参数向量](https://img-blog.csdnimg.cn/20190508205758750.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2EyNDUyOTMyMDY=,size_16,color_FFFFFF,t_70)

> 注意，因为想将𝑦<sup>^</sup>=w<sup>T</sup>X+b`转换`为𝑦<sup>^</sup>= 𝜎(𝜃
<sup>𝑇</sup>X)，`因为`𝜃增加一个偏差b，那么X也应`增加一维`x<sub>0</sub>=`1`，即转换成功

---

## 逻辑回归的代价函数（Logistic Regression Cost Function）

![逻辑回归图示](https://img-blog.csdnimg.cn/20190508211016817.png)

> 损失函数：用来`衡量`算法的运行情况(预测值和实际值有多`接近`)，Loss function:L(y<sup>^</sup>,y),一般我们用`预测值和实际值`的`平方差`或者`平方差的一半`，但在逻辑回归中我们`不`这么做（因为优化目标不是[凸优化](https://blog.csdn.net/xbinworld/article/details/79113218)（可行域及目标函数均凸），`只`能找到多个`局部`最优值，梯度下降法很可能找不到`全局`最优值），采用
![逻辑回归的损失函数](https://img-blog.csdnimg.cn/20190508211449152.png)

- 非凸优化

<img src="https://img-blog.csdn.net/20140107114211578" width="50%" height="50%">

- 凸优化

<img src="https://img-blog.csdn.net/20140107114309671" width="50%" height="50%">

- 为什么采用此损失函数呢？

![为什么采用此损失函数呢？](https://img-blog.csdnimg.cn/20190508212613797.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2EyNDUyOTMyMDY=,size_16,color_FFFFFF,t_70)

代价函数（损失函数的整体形式）就是：
![代价函数](https://img-blog.csdnimg.cn/20190508212807906.png)

我们在训练逻辑回归模型时，就是找到`合适的w和b`，让`代价函数`J最`低`

---
## 梯度下降法

**目前已经选取了假设函数和代价函数**

<img src="https://img-blog.csdnimg.cn/2019050821315331.png" width="60%" height="60%">

**梯度下降法的形象表示**

<img src="https://img-blog.csdnimg.cn/20190508213331422.png" width="30%">

`新`的成本函数较使得方差形式的`非凸`函数成为`凸`函数，而后由`初始`点（`随机`初始化）开始，沿着`梯度最大`的方向走一定的`步长`,`迭代`此过程，最后走到`全局最优`点


<img src="https://img-blog.csdnimg.cn/20190508214013359.png" width="150">

> α是学习率，用来`控制步长`

> 最后的除法公式就是一个`偏导`公式

---

## Summary Day03
```
今天开启了二分类这一章节，由第一节学习到了第六节，新学习到的概念有：
- 逻辑回归
- 分类与回归
- 维度
- sigmoid函数
- 假设函数
- 损失函数
- 代价函数
- 凸函数与非凸函数
- 梯度下降法

需要注意的是：
1.逻辑回归的假设函数在用于分类问题时，传统的形式往往存在值域的不匹配问题，需要将假设函数作为sigmoid函数的自变量来解决，使函数收敛在(0,1)之间

2.特征矩阵并不是将RGB以同等形式录入，而是将每一个颜色的强度值矩阵变为一维线性连接起来（此处存在理解错误，所以没弄明白维度）

3.𝜃矩阵的变换导致了X输入矩阵的变化，增加了一维

4.损失函数是单个样本，代价函数是样本总体损失函数值的平均值

5.非凸函数存在局部最优解问题，由平方差的形式变换为
𝐿(𝑦^ , 𝑦) = −𝑦log(𝑦^) − (1 − 𝑦)log(1 − 𝑦^)

```